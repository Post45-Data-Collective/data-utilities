{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Google Books by Title/Author\n",
    "\n",
    "This script will add various fields from Google Books results based on a title and author search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`title_column` - the name of the column header that contains the title\n",
    "`author_column` - the name of the column header that contains the author\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/nyt_hardcover_fiction_bestsellers-titles.tsv\"\n",
    "title_column = \"title\"\n",
    "author_column = \"author\"\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_google_books(d):\n",
    "\n",
    "\n",
    "\n",
    "    # if there is already a value skip it\n",
    "    if 'oclc_marc' in d:\n",
    "        if type(d['oclc_marc']) == str:        \n",
    "            print('Skip',d[id_column_name])\n",
    "            return d\n",
    "\n",
    "    # if you want to add any logic to only download some records add it here\n",
    "    if type(d['author_lccn']) == str:\n",
    "        print('Skip',d[id_column_name])\n",
    "        return d\n",
    "\n",
    "    try:\n",
    "        int(d[id_column_name])\n",
    "    except:\n",
    "        print(\"Cannot convert to int\",d[id_column_name])\n",
    "        return d\n",
    "    \n",
    "    url = f\"https://www.worldcat.org/webservices/catalog/content/{int(d[id_column_name])}?wskey={WSkey}\"\n",
    "    \n",
    "    r = requests.get(url, headers={'Accept': 'application/json', 'User-Agent': user_agent})\n",
    "    \n",
    "    if '<leader>' not in r.text:\n",
    "        print(\"No OCLC record XML:\",d[id_column_name])\n",
    "        return d\n",
    "    \n",
    "    d['oclc_marc'] = r.text\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.lower()\n",
    "    s = s.casefold()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = s.replace('the','')\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 537:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: download_google_books(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
