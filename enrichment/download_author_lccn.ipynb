{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author LCCN Download\n",
    "\n",
    "This script will talk to id.loc.gov to find the LCCN for an authorized author name heading. \n",
    "\n",
    "It is a requirement that you have a full name heading from the MARC record such as field 100. You can add these from Hathi trust data running the `parse_hathi_add_auth_name` script\n",
    "\n",
    "Since name string matching can be error prone if you have work titles in your dataset make sure to configure `use_title_reconcilation` and `title_column` to get better results by using the work title in reconciliation process.\n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It creates a new column in the file `author_lccn` with the LCCN value `author_authorized_heading` which will have the authorized string heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`id_column_name` - the name of the column header that contains authorized author heading value\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n",
    "\n",
    "`use_title_reconcilation` - boolean true/false to use a title to help reconcile non-exact matches, if you have a title in your dataset set this to true to get better results\n",
    "\n",
    "`title_column` - the name of the column that has the title to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/hathitrust_post45fiction_metadata.tsv\"\n",
    "id_column_name = \"author_marc\"\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 1\n",
    "\n",
    "use_title_reconcilation = True\n",
    "title_column = \"shorttitle\"\n",
    "\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lccn(d):\n",
    "    \n",
    "    # if type(d['oclc_marc']) != str:           \n",
    "    #     return d\n",
    "\n",
    "    if type(d[id_column_name]) != str:     \n",
    "        # no heading to use skipp\n",
    "        print(d)\n",
    "        return d\n",
    "        \n",
    "    # if there is already a value skip it\n",
    "    if 'author_lccn' in d:\n",
    "        if type(d['author_lccn']) == str:        \n",
    "            print('Skip',d[id_column_name])\n",
    "            return d\n",
    "\n",
    "\n",
    "    name = d[id_column_name]\n",
    "\n",
    "    # drop any trailing commas or periods\n",
    "    if name[-1] == '.'  or name[-1] == ',':\n",
    "        name = name[:-1]\n",
    "\n",
    "    # there is a common notation for the birth year that can be fixed easily ex: \"Hart, Frank W. (Frank William), b. 1881\" to \"Hart, Frank W. (Frank William), 1881\"\n",
    "    name = name.replace(', b. 1', ', 1')\n",
    "    \n",
    "    # keep a in memory cache to speed repeated requests up\n",
    "    if name in cache:\n",
    "        results = cache[name]\n",
    "    else:\n",
    "        params = {\n",
    "            'q' : name,\n",
    "            'count': 5\n",
    "        }\n",
    "        headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "        url = f\"https://id.loc.gov/authorities/names/suggest2/\"\n",
    "\n",
    "        r = requests.get(url,params=params,headers=headers)\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except:\n",
    "            print(\"JSON decode error with:\",d[id_column_name])\n",
    "            return d            \n",
    "\n",
    "        results = data['hits']\n",
    "        cache[name] = data['hits']\n",
    "    \n",
    "    # loop throguh each result and test the name\n",
    "    for hit in results:\n",
    "        if hit['suggestLabel'] == name:\n",
    "            d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "            d['author_authorized_heading'] = hit['aLabel']\n",
    "            return d\n",
    "    # check the main variant label \n",
    "    for hit in results:\n",
    "        if hit['vLabel'] == name:\n",
    "            d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "            d['author_authorized_heading'] = hit['aLabel']\n",
    "            return d\n",
    "\n",
    "    # if there is only one hit and it has unclosed life dates and the name partially matches then select it\n",
    "    if name[-1] == '-':\n",
    "        if len(results) == 1:\n",
    "            if name in results[0]['aLabel'] or name in results[0]['vLabel']:\n",
    "                d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "                d['author_authorized_heading'] = hit['aLabel']\n",
    "                return d\n",
    "\n",
    "    # if we are here then no match, loop again and look at the titles if enabled\n",
    "    if use_title_reconcilation == True:            \n",
    "        for hit in results:\n",
    "            url = 'https://id.loc.gov/resources/works/relationships/contributorto/'\n",
    "            params = {\n",
    "                'page': 0,\n",
    "                'label':hit['aLabel']\n",
    "            }\n",
    "            headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "\n",
    "            r = requests.get(url,params=params,headers=headers)\n",
    "            try:\n",
    "                title_data = r.json()\n",
    "            except:\n",
    "                print(\"JSON decode error with:\",d[id_column_name])\n",
    "                return d\n",
    "\n",
    "            if title_data['results'] != None:\n",
    "                # convert it to a list if it a single result dictonary\n",
    "                if type(title_data['results']) != list:\n",
    "                    title_data['results'] = [title_data['results']]\n",
    "                for title in title_data['results']:\n",
    "                    if normalize_string(d[title_column]) in normalize_string(title['label']):\n",
    "                        # we found the title hit, use this one\n",
    "                        d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "                        d['author_authorized_heading'] = hit['aLabel']\n",
    "\n",
    "                        return d\n",
    "\n",
    "        # often the wrong life dates are used but the main heading part is correct, so keep choping off the end of the heading and check it\n",
    "        # if we get a hit and then get a title match we can be confident it is correct. but it has to have a least 2 parts\n",
    "        # for example:\n",
    "        # \"Gorham, Charles O. (Charles Orson), 1911-\"\n",
    "        # \"Gorham, Charles O. (Charles Orson)\"\n",
    "        # \"Gorham, Charles O. (Charles\"\n",
    "        # \"Gorham, Charles O\" <- hits a result\n",
    "        for x in range(len(name.split())-1,1,-1):\n",
    "            cropped_name = \" \".join(name.split()[0:x])\n",
    "            if cropped_name[-1] == '.'  or cropped_name[-1] == ',':\n",
    "                cropped_name = cropped_name[:-1]\n",
    "            \n",
    "\n",
    "            params = {\n",
    "                'q' : cropped_name,\n",
    "                'count': 5\n",
    "            }\n",
    "            headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "            url = f\"https://id.loc.gov/authorities/names/suggest2/\"\n",
    "\n",
    "            r = requests.get(url,params=params,headers=headers)\n",
    "            try:\n",
    "                data = r.json()\n",
    "            except:\n",
    "                print(\"JSON decode error with:\",d[id_column_name])\n",
    "                return d\n",
    "            \n",
    "            if len(data['hits']) == 0:\n",
    "                continue\n",
    "\n",
    "            results = data['hits']\n",
    "            for hit in results:\n",
    "                url = 'https://id.loc.gov/resources/works/relationships/contributorto/'\n",
    "                params = {\n",
    "                    'page': 0,\n",
    "                    'label':hit['suggestLabel']\n",
    "                }\n",
    "                headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "\n",
    "                r = requests.get(url,params=params,headers=headers)\n",
    "                try:\n",
    "                    title_data = r.json()\n",
    "                except:\n",
    "                    print(\"JSON decode error with:\",d[id_column_name])\n",
    "                    return d\n",
    "\n",
    "                if title_data['results'] != None:\n",
    "                    # convert it to a list if it a single result dictonary\n",
    "                    if type(title_data['results']) != list:\n",
    "                        title_data['results'] = [title_data['results']]\n",
    "                    for title in title_data['results']:\n",
    "                        if normalize_string(d[title_column]) in normalize_string(title['label']):\n",
    "                            # we found the title hit, use this one\n",
    "                            d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "                            d['author_authorized_heading'] = hit['aLabel']\n",
    "                            print(\"Found\",name,\"using cropped\", cropped_name)\n",
    "                            return d\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"No results for \",d[id_column_name])\n",
    "    \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.lower()\n",
    "    s = s.casefold()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = s.replace('the','')\n",
    "    return s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 88:\n",
    "    #     continue\n",
    "\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_lccn(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This last block just does QA on the data to see what if any rows were not populated\n",
    "\n",
    "# df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "# print(\"There are \", df['hathi_marc'].isnull().any().sum(), 'rows with no hathi_marc column populated, here are their', id_column_name, 'values:')\n",
    "# res = df.loc[df['hathi_marc'].isnull(), id_column_name].tolist()\n",
    "# print(print(res))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
