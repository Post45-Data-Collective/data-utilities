{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCLC Download Classify\n",
    "\n",
    "This script adds the Classify xml blob from the OCLC API to a TSV file, it is a preparatory script to run other scripts to extract metadata from the Classify data. \n",
    "\n",
    "It is a requirement that you have either the OCLC number in the data \n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It creates a new column in the file `oclc_classify` which holds the MARC XML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`id_column_name` - the name of the column header that contains the oclc record id number\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n",
    "\n",
    "`WSKey` - the OCLC WSkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/nyt_hardcover_fiction_bestsellers-titles.tsv\"\n",
    "id_column_name = \"oclc\"\n",
    "id_author_name = 'author'\n",
    "id_title_name = 'title'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "WSkey = \"xxxx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_oclc(d):\n",
    "\n",
    "\n",
    "\n",
    "    # if there is already a value skip it\n",
    "    if 'oclc_classify' in d:\n",
    "        if type(d['oclc_classify']) == str:        \n",
    "            print('Skip',d[id_column_name])\n",
    "            return d\n",
    "\n",
    "    # # if you want to add any logic to only download some records add it here\n",
    "    # if type(d['author_lccn']) == str:\n",
    "    #     print('Skip',d[id_column_name])\n",
    "    #     return d\n",
    "\n",
    "    headers = {'X-OCLC-API-Key': WSkey}\n",
    "    params = {'author': d[id_author_name], 'title': d[id_title_name], 'summary' : 'false', 'maxRecs':100}\n",
    "    r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "    work_parsed = None\n",
    "    work_unparsed = None\n",
    "\n",
    "    if r.text.find('<response code=\"2\"/>') > -1:\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"4\"/>') > -1:\n",
    "        \n",
    "        soup = BeautifulSoup(str(r.text))\n",
    "        work_soup = soup.find(\"works\")\n",
    "        largest_count = 0\n",
    "        largest_work = None\n",
    "        for work in work_soup.find_all(\"work\"):\n",
    "            if int(work['holdings']) > largest_count:\n",
    "                largest_count = int(work['holdings'])\n",
    "                largest_work = work\n",
    "\n",
    "\n",
    "        print(largest_work)\n",
    "        params = {'owi': largest_work['owi'], 'summary' : 'false', 'maxRecs':100}\n",
    "        r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"100\"/>') > -1 or r.text.find('<response code=\\\\\"100\\\\\"/>') >-1:\n",
    "        print(params,'100: No input. The method requires an input argument.')\n",
    "    elif r.text.find('<response code=\"101\"/>') > -1 or r.text.find('<response code=\\\\\"101\\\\\"/>') >-1:\n",
    "        print(params,'101: Invalid input. The standard number argument is invalid.')\n",
    "    elif r.text.find('<response code=\"102\"/>') > -1 or r.text.find('<response code=\\\\\"102\\\\\"/>') >-1:\n",
    "        print(params,'102: ?.')\n",
    "    elif r.text.find('<response code=\"200\"/>') > -1 or r.text.find('<response code=\\\\\"200\\\\\"/>') >-1:\n",
    "        print(params,'200: Unexpected error.')\n",
    "    else:\n",
    "        print(\"unknown Problem:\",r.text)\n",
    "\n",
    "\n",
    "    if work_parsed != None:\n",
    "\n",
    "        d['oclc_classify'] = work_unparsed\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify(xml):\n",
    "\n",
    "\n",
    "\t\tsoup = BeautifulSoup(str(xml))\n",
    "\n",
    "\t\twork_soup = soup.find(\"work\")\n",
    "\n",
    "\t\tif work_soup == None:\n",
    "\t\t\t# print(\"can not parse xml:\")\n",
    "\t\t\t# print(xml)\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tresults = {}\n",
    "\n",
    "\t\tresults['work_statement_responsibility'] = None if work_soup.has_attr('author') == False else work_soup['author']\n",
    "\t\tresults['work_editions'] = None if work_soup.has_attr('editions') == False else int(work_soup['editions'])\n",
    "\t\tresults['work_eholdings'] = None if work_soup.has_attr('eholdings') == False else int(work_soup['eholdings'])\n",
    "\t\tresults['work_format'] = None if work_soup.has_attr('format') == False else work_soup['format']\n",
    "\t\tresults['work_holdings'] = None if work_soup.has_attr('holdings') == False else int(work_soup['holdings'])\n",
    "\t\tresults['work_itemtype'] = None if work_soup.has_attr('itemtype') == False else work_soup['itemtype']\n",
    "\t\tresults['work_owi'] = None if work_soup.has_attr('owi') == False else work_soup['owi']\n",
    "\t\tresults['work_title'] = None if work_soup.has_attr('title') == False else work_soup['title']\n",
    "\t\tresults['main_oclc'] = work_soup.text\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tauthors_soup = soup.find_all(\"author\")\n",
    "\t\tresults['authors'] = []\n",
    "\t\tfor a in authors_soup:\n",
    "\t\t\tresults['authors'].append({\n",
    "\t\t\t\t\t\"name\" : a.text.split('[')[0].strip(),\n",
    "\t\t\t\t\t\"lccn\" : None if a.has_attr('lc') == False else a['lc'],\n",
    "\t\t\t\t\t\"viaf\" : None if a.has_attr('viaf') == False else a['viaf']\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\tfor a in results['authors']:\n",
    "\t\t\tif a['lccn'] == 'null':\n",
    "\t\t\t\ta['lccn'] = None\t\n",
    "\t\t\tif a['viaf'] == 'null':\n",
    "\t\t\t\ta['viaf'] = None\t\n",
    "\n",
    "\t\t# try to find the first main contributor\n",
    "\t\tresults['work_author'] = None\n",
    "\t\tif results['work_statement_responsibility'] != None:\n",
    "\t\t\tif len(results['work_statement_responsibility'].split(\"|\"))>0:\n",
    "\t\t\t\tfirst_author = results['work_statement_responsibility'].split(\"|\")[0].strip()\n",
    "\t\t\t\tfor a in results['authors']:\n",
    "\t\t\t\t\tprint(a['name'].split('[')[0].strip(), first_author )\n",
    "\t\t\t\t\tif a['name'].strip() == first_author:\n",
    "\t\t\t\t\t\tresults['work_author'] = a\n",
    "\n",
    "\n",
    "\n",
    "\t\tresults[\"normalized_ddc\"] = None\n",
    "\t\tresults[\"normalized_lcc\"] = None\n",
    "\n",
    "\t\tddc_soup = soup.find(\"ddc\")\n",
    "\t\tif ddc_soup != None:\n",
    "\t\t\tddc_soup = soup.find(\"ddc\").find(\"mostpopular\")\n",
    "\t\t\tif ddc_soup != None:\n",
    "\t\t\t\tif ddc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_ddc\"] = ddc_soup['nsfa']\n",
    "\n",
    "\t\tlcc_soup = soup.find(\"lcc\")\n",
    "\t\tif lcc_soup != None:\n",
    "\t\t\tlcc_soup = soup.find(\"lcc\").find(\"mostpopular\")\n",
    "\t\t\tif lcc_soup != None:\n",
    "\t\t\t\tif lcc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_lcc\"] = lcc_soup['nsfa']\n",
    "\n",
    "\n",
    "\t\tresults[\"headings\"] = []\n",
    "\t\theading_soup = soup.find_all(\"heading\")\n",
    "\t\tfor h in heading_soup:\n",
    "\t\t\tresults[\"headings\"].append({\n",
    "\t\t\t\t\t\"id\" : h['ident'],\n",
    "\t\t\t\t\t\"src\": h['src'],\n",
    "\t\t\t\t\t\"value\" : h.text\n",
    "\t\t\t\t})\n",
    "\t\t\t\n",
    "\n",
    "\t\tedition_soup = soup.find_all(\"edition\")\n",
    "\t\t# print(isbn,len(edition_soup))\n",
    "\t\tresults[\"editions\"] = []\n",
    "\t\tfor e in edition_soup:\n",
    "\t\t\tedition = {}\n",
    "\t\t\tedition['author'] = None if e.has_attr('author') == False else e['author']\n",
    "\t\t\tedition['eholdings'] = None if e.has_attr('eholdings') == False else int(e['eholdings'])\n",
    "\t\t\tedition['format'] = None if e.has_attr('format') == False else e['format']\n",
    "\t\t\tedition['holdings'] = None if e.has_attr('holdings') == False else int(e['holdings'])\n",
    "\t\t\tedition['itemtype'] = None if e.has_attr('itemtype') == False else e['itemtype']\n",
    "\t\t\tedition['language'] = None if e.has_attr('language') == False else e['language']\n",
    "\t\t\tedition['oclc'] = None if e.has_attr('oclc') == False else e['oclc']\n",
    "\t\t\tedition['title'] = None if e.has_attr('title') == False else e['title']\n",
    "\t\t\tresults[\"editions\"].append(edition)\n",
    "\t\t\n",
    "\t\tif len(results[\"editions\"]) > 0:\n",
    "\t\t\tresults[\"largest_holding_oclc\"] = results[\"editions\"][0]['oclc']\n",
    "\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 707:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_oclc(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
