{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add missing LCCN / VIAF / Wikidata\n",
    "\n",
    "This script adds the LCCN / VIAF / Wikidata identifier and Authorized Heading Label if one of the other identifiers is populated. \n",
    "\n",
    "It is a requirement that you have at least the LCCN or VIAF\n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It creates a new column in the file `author_viaf` `author_viaf` `author_wikidata` `author_authorized_heading` which holds the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n",
    "\n",
    "`author_viaf` - column where viaf number is stored\n",
    "\n",
    "`author_lccn` - column where lccn  number is stored\n",
    "\n",
    "`author_wikidata` - column where qid number is stored\n",
    "\n",
    "`author_authorized_heading` - the column where the authorized name is stored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/nyt_hardcover_fiction_bestsellers-hathitrust_metadata.tsv\"\n",
    "\n",
    "user_agent = 'USER YOUR_USER_NAME - Test Script'\n",
    "pause_between_req = 0\n",
    "\n",
    "author_viaf = 'author_viaf'\n",
    "author_lccn = 'author_lccn'\n",
    "author_wikidata = 'wikidata_qid'\n",
    "author_authorized_heading = 'author_authorized_heading'\n",
    "\n",
    "wikidata_cache={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qid(d):\n",
    "\n",
    "    if author_wikidata not in d:\n",
    "        d[author_wikidata] = None\n",
    "\n",
    "    if author_viaf not in d:\n",
    "        d[author_viaf] = None\n",
    "\n",
    "    if author_lccn not in d:\n",
    "        d[author_lccn] = None\n",
    "\n",
    "        \n",
    "    # does it have LCCN but no VIAF \n",
    "    if pd.isnull(d[author_viaf]) == True and type(d[author_lccn]) == str:\n",
    "        print(\"Try to get VIAF from LCCN\", d[author_lccn])\n",
    "\n",
    "        headers={'User-Agent': user_agent}\n",
    "        url = f\"https://viaf.org/viaf/sourceID/LC%7C{d[author_lccn]}\"\n",
    "        r = requests.get(url,headers=headers,allow_redirects=False)\n",
    "        if r.status_code == 404:\n",
    "            return d\n",
    "\n",
    "        viaf = r.headers['Location'].split('/')[-1]\n",
    "        print(\"Found VIAF viaf LCCN\",viaf)\n",
    "        d[author_viaf] = viaf\n",
    "\n",
    "    # does it have VIAF but no LCCN\n",
    "    if pd.isnull(d[author_viaf]) != True and pd.isnull(d[author_lccn]) == True:\n",
    "\n",
    "\n",
    "        print(\"Try to get LCCN from VIAF\", d[author_viaf])\n",
    "        headers={'User-Agent': user_agent}\n",
    "        url = f\"https://www.viaf.org/viaf/{d[author_viaf]}/?httpAccept=application/json\"\n",
    "        r = requests.get(url,headers=headers)\n",
    "        if r.status_code != 404:           \n",
    "            \n",
    "            data = r.json()\n",
    "            if type(data['sources']['source']) != list:\n",
    "                data['sources']['source'] = [data['sources']['source']]\n",
    "\n",
    "            for source in data['sources']['source']:\n",
    "                if source['#text'][0:3] == 'LC|':\n",
    "                    print(\"FOOND LC!\", source['#text'].replace(' ','').split('|')[1])\n",
    "                    d[author_lccn] = source['#text'].replace(' ','').split('|')[1]\n",
    "\n",
    "    # does it not have a wikidata but it has a lccn?\n",
    "    if pd.isnull(d[author_wikidata]) == True and pd.isnull(d[author_lccn]) == False:\n",
    "\n",
    "        if d[author_lccn] in wikidata_cache:\n",
    "            d[author_wikidata] = wikidata_cache[d[author_lccn]]\n",
    "        else:    \n",
    "\n",
    "\n",
    "            sparql = f\"\"\"\n",
    "                SELECT ?item ?itemLabel\n",
    "                WHERE \n",
    "                {{\n",
    "                ?item wdt:P244 \"{d[author_lccn]}\".\n",
    "                SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "                }}\n",
    "            \"\"\"\n",
    "            params = {\n",
    "                'query' : sparql\n",
    "            }\n",
    "\n",
    "            headers = {\n",
    "                'Accept' : 'application/json',\n",
    "                'User-Agent': user_agent\n",
    "            }\n",
    "            url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "            r = requests.get(url, params=params, headers=headers)\n",
    "            data = r.json()\n",
    "\n",
    "            # did we get any results\n",
    "            if len(data['results']['bindings']) > 0:\n",
    "                # the qid is part of the URI, chop off the identifier       \n",
    "                d[author_wikidata] = data['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
    "                wikidata_cache[d[author_lccn]] = d[author_wikidata] \n",
    "                print(\"Found wikidata via LCCN\", d[author_wikidata])\n",
    "\n",
    "    # does it not have a wikidata but it has a viaf?\n",
    "    if pd.isnull(d[author_wikidata]) == True and pd.isnull(d[author_viaf]) == False:\n",
    "        sparql = f\"\"\"\n",
    "            SELECT ?item ?itemLabel\n",
    "            WHERE \n",
    "            {{\n",
    "            ?item wdt:P214 \"{d[author_viaf]}\".\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'query' : sparql\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'Accept' : 'application/json',\n",
    "            'User-Agent': user_agent\n",
    "        }\n",
    "        url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        data = r.json()\n",
    "\n",
    "        # did we get any results\n",
    "        if len(data['results']['bindings']) > 0:\n",
    "            # the qid is part of the URI, chop off the identifier       \n",
    "            d[author_wikidata] = data['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
    "            print(\"Found wikidata via VIAF\", d[author_wikidata])\n",
    "    \n",
    "    # does not have the authorized heading but it has a LCCN?\n",
    "    if pd.isnull(d[author_authorized_heading]) == True and pd.isnull(d[author_lccn]) == False:\n",
    "\n",
    "\n",
    "        headers={'User-Agent': user_agent}\n",
    "        url = f\"https://id.loc.gov/authorities/names/suggest2/?q={d[author_lccn]}\"\n",
    "        r = requests.get(url,headers=headers)\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        if data['count'] == 0:\n",
    "            print(\"Bad LCCN\",d[author_lccn] )\n",
    "\n",
    "        d[author_authorized_heading] = data['hits'][0]['aLabel']\n",
    "\n",
    "\n",
    "\n",
    "    # does not have the authorized heading but it has a VIAF?\n",
    "    if pd.isnull(d[author_authorized_heading]) == True and pd.isnull(d[author_viaf]) == False:\n",
    "\n",
    "\n",
    "        headers={'User-Agent': user_agent}\n",
    "        url = f\"https://viaf.org/viaf/{d[author_viaf]}/viaf.json\"\n",
    "        r = requests.get(url,headers=headers)\n",
    "        if r.status_code != 404:                       \n",
    "            data = r.json()\n",
    "\n",
    "            # if the cluster is redirecting reload the new destination \n",
    "            if 'redirect' in data:\n",
    "                url = f\"https://viaf.org/viaf/{data['redirect']['directto']}/viaf.json\"\n",
    "                r = requests.get(url,headers=headers)\n",
    "                data = r.json()\n",
    "            \n",
    "            # dunno what this is, sometimes the record is nested in this key\n",
    "            if 'scavenged' in data:\n",
    "                data = data['scavenged']['VIAFCluster']\n",
    "\n",
    "            if type(data['mainHeadings']['data']) != list:\n",
    "                data['mainHeadings']['data'] = [data['mainHeadings']['data']]\n",
    "\n",
    "            d[author_authorized_heading] = data['mainHeadings']['data'][0]['text']\n",
    "\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_qid(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "df_dropped = df.dropna(subset=[author_viaf,author_lccn,author_wikidata], how='all')\n",
    "\n",
    "print('Number of rows with one identifier populated:',len(df_dropped.index), 'out of ',len(df.index), len(df_dropped.index)/len(df.index)*100 )\n",
    "\n",
    "df_dropped = df.dropna(subset=[author_viaf], how='all')\n",
    "print('Number of rows with author_viaf populated:',len(df_dropped.index), 'out of ',len(df.index), len(df_dropped.index)/len(df.index)*100 )\n",
    "\n",
    "df_dropped = df.dropna(subset=[author_lccn], how='all')\n",
    "print('Number of rows with author_lccn populated:',len(df_dropped.index), 'out of ',len(df.index), len(df_dropped.index)/len(df.index)*100 )\n",
    "\n",
    "df_dropped = df.dropna(subset=[author_wikidata], how='all')\n",
    "print('Number of rows with author_wikidata populated:',len(df_dropped.index), 'out of ',len(df.index), len(df_dropped.index)/len(df.index)*100 )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
