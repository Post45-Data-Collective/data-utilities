{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add OL Work ID\n",
    "\n",
    "This script expects you have downloaded the Open Library editions file: https://openlibrary.org/developers/dumps\n",
    "It will first loop through you data and build a lookup for all the ids its has for your data, then loop through the ol dump and see what matches up\n",
    "Fill out the column names below, if you don't have a piece of the data set it equal to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/hathitrust_post45fiction_metadata.tsv\"\n",
    "path_to_ol_editions = \"/Users/m/Downloads/ol_dump_editions_2023-07-31.txt\"\n",
    "\n",
    "record_id = \"recordid\" # this is the id that will be used to backfill the data into your data, should be populated for all records\n",
    "\n",
    "oclc_classify = \"oclc_classify\" # if you have the classify xml blob else set to None\n",
    "oclc = \"oclc\"\n",
    "isbns = \"isbns\"\n",
    "\n",
    "lookup_oclc = {}\n",
    "lookup_isbn = {}\n",
    "record_id_lookup = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classify_row(d):\n",
    "\n",
    "    # do we have some classify data?\n",
    "    if oclc_classify != None:\n",
    "\n",
    "        if type(d[oclc_classify]) == str:   \n",
    "\n",
    "            data = extract_classify(d[oclc_classify])\n",
    "            if data != None:\n",
    "                \n",
    "                for e in data[\"editions\"]:\n",
    "\n",
    "                    o = str(e['oclc'])\n",
    "                    lookup_oclc[o] = d[record_id]\n",
    "\n",
    "\n",
    "            else:\n",
    "                #print(\"bad data\", d['oclc_classify'])\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # print(\"No Classify data to parse:\",d)\n",
    "            pass\n",
    "\n",
    "        \n",
    "    if oclc != None:            \n",
    "        if pd.isnull(d[oclc]) == False:\n",
    "\n",
    "            o = str(int(d[oclc]))\n",
    "            lookup_oclc[o] = d[record_id]\n",
    "    \n",
    "    if isbns != None:\n",
    "        if pd.isnull(d[isbns]) == False:\n",
    "\n",
    "            isbn_numbers = d[isbns].split(\"|\")\n",
    "            for i in isbn_numbers:\n",
    "                lookup_isbn[i] = d[record_id]\n",
    "\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ol_work(d):\n",
    "\n",
    "    \n",
    "    if str(d[record_id]) in record_id_lookup:\n",
    "        d['ol_work'] = record_id_lookup[str(d[record_id])][0]['key']\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify(xml):\n",
    "\n",
    "\n",
    "\t\tsoup = BeautifulSoup(str(xml))\n",
    "\n",
    "\t\twork_soup = soup.find(\"work\")\n",
    "\n",
    "\t\tif work_soup == None:\n",
    "\t\t\t# print(\"can not parse xml:\")\n",
    "\t\t\t# print(xml)\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tresults = {}\n",
    "\n",
    "\t\tresults['work_statement_responsibility'] = None if work_soup.has_attr('author') == False else work_soup['author']\n",
    "\t\tresults['work_editions'] = None if work_soup.has_attr('editions') == False else int(work_soup['editions'])\n",
    "\t\tresults['work_eholdings'] = None if work_soup.has_attr('eholdings') == False else int(work_soup['eholdings'])\n",
    "\t\tresults['work_format'] = None if work_soup.has_attr('format') == False else work_soup['format']\n",
    "\t\tresults['work_holdings'] = None if work_soup.has_attr('holdings') == False else int(work_soup['holdings'])\n",
    "\t\tresults['work_itemtype'] = None if work_soup.has_attr('itemtype') == False else work_soup['itemtype']\n",
    "\t\tresults['work_owi'] = None if work_soup.has_attr('owi') == False else work_soup['owi']\n",
    "\t\tresults['work_title'] = None if work_soup.has_attr('title') == False else work_soup['title']\n",
    "\t\tresults['main_oclc'] = work_soup.text\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tauthors_soup = soup.find_all(\"author\")\n",
    "\t\tresults['authors'] = []\n",
    "\t\tfor a in authors_soup:\n",
    "\t\t\tresults['authors'].append({\n",
    "\t\t\t\t\t\"name\" : a.text,\n",
    "\t\t\t\t\t\"lccn\" : None if a.has_attr('lc') == False else a['lc'],\n",
    "\t\t\t\t\t\"viaf\" : None if a.has_attr('viaf') == False else a['viaf']\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\tfor a in results['authors']:\n",
    "\t\t\tif a['lccn'] == 'null':\n",
    "\t\t\t\ta['lccn'] = None\t\n",
    "\t\t\tif a['viaf'] == 'null':\n",
    "\t\t\t\ta['viaf'] = None\t\n",
    "\n",
    "\t\t# try to find the first main contributor\n",
    "\t\tresults['work_author'] = None\n",
    "\t\tif results['work_statement_responsibility'] != None:\n",
    "\t\t\tif len(results['work_statement_responsibility'].split(\"|\"))>0:\n",
    "\t\t\t\tfirst_author = results['work_statement_responsibility'].split(\"|\")[0].strip()\n",
    "\t\t\t\tfor a in results['authors']:\n",
    "\t\t\t\t\tif a['name'].strip() == first_author:\n",
    "\t\t\t\t\t\tresults['work_author'] = a\n",
    "\n",
    "\n",
    "\n",
    "\t\tresults[\"normalized_ddc\"] = None\n",
    "\t\tresults[\"normalized_lcc\"] = None\n",
    "\n",
    "\t\tddc_soup = soup.find(\"ddc\")\n",
    "\t\tif ddc_soup != None:\n",
    "\t\t\tddc_soup = soup.find(\"ddc\").find(\"mostpopular\")\n",
    "\t\t\tif ddc_soup != None:\n",
    "\t\t\t\tif ddc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_ddc\"] = ddc_soup['nsfa']\n",
    "\n",
    "\t\tlcc_soup = soup.find(\"lcc\")\n",
    "\t\tif lcc_soup != None:\n",
    "\t\t\tlcc_soup = soup.find(\"lcc\").find(\"mostpopular\")\n",
    "\t\t\tif lcc_soup != None:\n",
    "\t\t\t\tif lcc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_lcc\"] = lcc_soup['nsfa']\n",
    "\n",
    "\n",
    "\t\tresults[\"headings\"] = []\n",
    "\t\theading_soup = soup.find_all(\"heading\")\n",
    "\t\tfor h in heading_soup:\n",
    "\t\t\tresults[\"headings\"].append({\n",
    "\t\t\t\t\t\"id\" : h['ident'],\n",
    "\t\t\t\t\t\"src\": h['src'],\n",
    "\t\t\t\t\t\"value\" : h.text\n",
    "\t\t\t\t})\n",
    "\t\t\t\n",
    "\n",
    "\t\tedition_soup = soup.find_all(\"edition\")\n",
    "\t\t# print(isbn,len(edition_soup))\n",
    "\t\tresults[\"editions\"] = []\n",
    "\t\tfor e in edition_soup:\n",
    "\t\t\tedition = {}\n",
    "\t\t\tedition['author'] = None if e.has_attr('author') == False else e['author']\n",
    "\t\t\tedition['eholdings'] = None if e.has_attr('eholdings') == False else int(e['eholdings'])\n",
    "\t\t\tedition['format'] = None if e.has_attr('format') == False else e['format']\n",
    "\t\t\tedition['holdings'] = None if e.has_attr('holdings') == False else int(e['holdings'])\n",
    "\t\t\tedition['itemtype'] = None if e.has_attr('itemtype') == False else e['itemtype']\n",
    "\t\t\tedition['language'] = None if e.has_attr('language') == False else e['language']\n",
    "\t\t\tedition['oclc'] = None if e.has_attr('oclc') == False else e['oclc']\n",
    "\t\t\tedition['title'] = None if e.has_attr('title') == False else e['title']\n",
    "\t\t\tresults[\"editions\"].append(edition)\n",
    "\t\t\n",
    "\t\tif len(results[\"editions\"]) > 0:\n",
    "\t\t\tresults[\"largest_holding_oclc\"] = results[\"editions\"][0]['oclc']\n",
    "\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "# df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "# df.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "\n",
    "# # run our function over all of them to build the lookups\n",
    "# df = df.apply(lambda d: parse_classify_row(d),axis=1 )  \n",
    "\n",
    "\n",
    "# print('lookup_oclc',len(lookup_oclc))\n",
    "# print('lookup_isbn',len(lookup_isbn))\n",
    "# counter = 0\n",
    "# with open(path_to_ol_editions) as ol_file:\n",
    "\n",
    "#     for line in ol_file:\n",
    "#         counter=counter+1\n",
    "#         if counter % 1000000 == 0:\n",
    "#             print(counter)\n",
    "\n",
    "#         json_text = line.split(\"\\t\")[4]\n",
    "#         try:\n",
    "#             data = json.loads(json_text)\n",
    "#         except:\n",
    "#             print(\"Bad parse:\", json_text)\n",
    "#         if 'works' in data:\n",
    "                \n",
    "#             if 'oclc_numbers' in data:\n",
    "#                 for o in data['oclc_numbers']:\n",
    "#                     if o in lookup_oclc:\n",
    "#                         record_id_lookup[lookup_oclc[o]] = data['works']\n",
    "#             if 'isbn_10' in data:\n",
    "#                 for i in data['isbn_10']:\n",
    "#                     if i in lookup_isbn:\n",
    "#                         record_id_lookup[lookup_isbn[i]] = data['works']\n",
    "#             if 'isbn_13' in data:\n",
    "#                 for i in data['isbn_13']:\n",
    "#                     if i in lookup_isbn:\n",
    "#                         record_id_lookup[lookup_isbn[i]] = data['works']\n",
    "\n",
    "\n",
    "# json.dump(record_id_lookup, open('record_id_lookup.json','w'))\n",
    "record_id_lookup = json.load(open('record_id_lookup.json'))\n",
    "print('record_id_lookup',len(record_id_lookup))\n",
    "\n",
    "df2 = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df2.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "# add in our data based on record_id\n",
    "df2 = df2.apply(lambda d: add_ol_work(d),axis=1 )  \n",
    "# overwrite back out\n",
    "df2.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
