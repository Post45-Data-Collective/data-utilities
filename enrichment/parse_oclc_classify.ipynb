{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCLC Parse Classify\n",
    "\n",
    "This script expects to work on a TSV that has MARC XML in a `oclc_classify` column, likely added by the `download_oclc_classify` script previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/nyt_hardcover_fiction_bestsellers-hathitrust_metadata.tsv\"\n",
    "author_viaf = 'author_viaf'\n",
    "author_lccn = 'author_lccn'\n",
    "work_title = 'shorttitle' # or set to None\n",
    "author_marc = 'author_marc' # the athuor name from the marc record\n",
    "author_authorized_heading = 'author_authorized_heading'\n",
    "\n",
    "\n",
    "# isbn_cache = json.load(open('/Users/m/Downloads/oclc_numbers.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classify_row(d):\n",
    "\n",
    "    # do we have some classify data?\n",
    "    if type(d['oclc_classify']) == str:   \n",
    "\n",
    "\n",
    "        data = extract_classify(d['oclc_classify'])\n",
    "        if data != None:\n",
    "\n",
    "            # # the titles are too different on many levels, not a good check\n",
    "            # # a little sanity check here, if we have the title, compare and make sure they look good\n",
    "            # if work_title != None and pd.isnull(d[work_title]) == False:\n",
    "\n",
    "            #     #which is shortest\n",
    "            #     if len(data['work_title']) >=  len(str(d[work_title])):\n",
    "            #         shortest = len(str(d[work_title]))\n",
    "            #     else:\n",
    "            #         shortest = len(data['work_title'])\n",
    "\n",
    "            #     shorttitle = data['work_title'][0:shortest]\n",
    "            #     shorttitle2 = str(d[work_title])[0:shortest]\n",
    "\n",
    "            #     title_score = levenshtein(shorttitle,shorttitle2)\n",
    "            #     if title_score > 20:\n",
    "            #         print(shorttitle)\n",
    "            #         print(shorttitle2)\n",
    "            #         print('-------')\n",
    "\n",
    "            # # this test showed lots of errors in the orginal data/marc record, nothing much more, not needed\n",
    "            # if pd.isnull(d[author_viaf]) == True and data['work_author'] != None:\n",
    "            #     if data['work_author']['viaf'] != None:\n",
    "\n",
    "            #         #which is shortest\n",
    "            #         if len(data['work_author']['name']) >=  len(str(d[author_marc])):\n",
    "            #             shortest = len(str(d[author_marc]))\n",
    "            #         else:\n",
    "            #             shortest = len(data['work_author']['name'])\n",
    "\n",
    "\n",
    "            #         shortauthor = data['work_author']['name'][0:shortest]\n",
    "            #         shortauthor2 = str(d[author_marc])[0:shortest]\n",
    "\n",
    "\n",
    "            #         if levenshtein(shortauthor,shortauthor2) >= shortest / 2:\n",
    "            #             print(\"HERE HEREEHEH\",data['work_author']['name'], \"!=\", d[author_marc], \"|\", d['oclc'] )\n",
    "\n",
    "            # add in the author info if is missing\n",
    "            if author_viaf not in d:\n",
    "                d[author_viaf] = None\n",
    "            if author_lccn not in d:\n",
    "                d[author_lccn] = None   \n",
    "                             \n",
    "            if pd.isnull(d[author_viaf]) == True and data['work_author'] != None:\n",
    "                if data['work_author']['viaf'] != None:\n",
    "                    d[author_authorized_heading] = data['work_author']['name']\n",
    "                    d[author_viaf] = data['work_author']['viaf']\n",
    "           \n",
    "            if pd.isnull(d[author_lccn]) == True and data['work_author'] != None:\n",
    "                if data['work_author']['lccn'] != None:                    \n",
    "                    d[author_lccn] = data['work_author']['lccn']\n",
    "                    d[author_authorized_heading] = data['work_author']['name']\n",
    "                    print(data['work_author'])\n",
    "\n",
    "\n",
    "            \n",
    "            # # d['oclc_largest_holding'] = data['largest_holding_oclc']\n",
    "            # all_isbns = []\n",
    "            # for e in data[\"editions\"]:\n",
    "            #     oclc = str(int(e['oclc']))\n",
    "            #     if oclc in isbn_cache:\n",
    "            #         if isbn_cache[oclc] != None:\n",
    "                            \n",
    "            #             cachedata = isbn_cache[oclc]\n",
    "            #             if 'workExample' in cachedata:\n",
    "            #                 if type(cachedata['workExample']) != list:\n",
    "            #                     cachedata['workExample'] = [cachedata['workExample']]\n",
    "                            \n",
    "            #                 isbns = []\n",
    "            #                 for url in cachedata['workExample']:\n",
    "            #                     if 'isbn' in url:\n",
    "            #                         isbns.append(url.split('/')[-1])\n",
    "            #                 all_isbns = all_isbns + isbns\n",
    "                            \n",
    "            # all_isbns = list(set(all_isbns))\n",
    "            # d['isbns'] = \"|\".join(all_isbns)\n",
    "\n",
    "            d['oclc_eholdings'] = data['work_eholdings']\n",
    "            d['oclc_holdings'] = data['work_holdings']\n",
    "            d['oclc_owi'] = data['work_owi']\n",
    "            \n",
    "\n",
    "        else:\n",
    "            #print(\"bad data\", d['oclc_classify'])\n",
    "            pass\n",
    "\n",
    "        \n",
    "\n",
    "    else:\n",
    "        # print(\"No Classify data to parse:\",d)\n",
    "        pass\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify(xml):\n",
    "\n",
    "\n",
    "\t\tsoup = BeautifulSoup(str(xml))\n",
    "\n",
    "\t\twork_soup = soup.find(\"work\")\n",
    "\n",
    "\t\tif work_soup == None:\n",
    "\t\t\t# print(\"can not parse xml:\")\n",
    "\t\t\t# print(xml)\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tresults = {}\n",
    "\n",
    "\t\tresults['work_statement_responsibility'] = None if work_soup.has_attr('author') == False else work_soup['author']\n",
    "\t\tresults['work_editions'] = None if work_soup.has_attr('editions') == False else int(work_soup['editions'])\n",
    "\t\tresults['work_eholdings'] = None if work_soup.has_attr('eholdings') == False else int(work_soup['eholdings'])\n",
    "\t\tresults['work_format'] = None if work_soup.has_attr('format') == False else work_soup['format']\n",
    "\t\tresults['work_holdings'] = None if work_soup.has_attr('holdings') == False else int(work_soup['holdings'])\n",
    "\t\tresults['work_itemtype'] = None if work_soup.has_attr('itemtype') == False else work_soup['itemtype']\n",
    "\t\tresults['work_owi'] = None if work_soup.has_attr('owi') == False else work_soup['owi']\n",
    "\t\tresults['work_title'] = None if work_soup.has_attr('title') == False else work_soup['title']\n",
    "\t\tresults['main_oclc'] = work_soup.text\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tauthors_soup = soup.find_all(\"author\")\n",
    "\t\tresults['authors'] = []\n",
    "\t\tfor a in authors_soup:\n",
    "\t\t\tresults['authors'].append({\n",
    "\t\t\t\t\t\"name\" : a.text.split('[')[0].strip(),\n",
    "\t\t\t\t\t\"lccn\" : None if a.has_attr('lc') == False else a['lc'],\n",
    "\t\t\t\t\t\"viaf\" : None if a.has_attr('viaf') == False else a['viaf']\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\tfor a in results['authors']:\n",
    "\t\t\tif a['lccn'] == 'null':\n",
    "\t\t\t\ta['lccn'] = None\t\n",
    "\t\t\tif a['viaf'] == 'null':\n",
    "\t\t\t\ta['viaf'] = None\t\n",
    "\n",
    "\t\t# try to find the first main contributor\n",
    "\t\tresults['work_author'] = None\n",
    "\t\tif results['work_statement_responsibility'] != None:\n",
    "\t\t\tif len(results['work_statement_responsibility'].split(\"|\"))>0:\n",
    "\t\t\t\tfirst_author = results['work_statement_responsibility'].split(\"|\")[0].strip()\n",
    "\t\t\t\tfor a in results['authors']:\n",
    "\t\t\t\t\tif a['name'].strip() == first_author:\n",
    "\t\t\t\t\t\tresults['work_author'] = a\n",
    "\n",
    "\n",
    "\n",
    "\t\tresults[\"normalized_ddc\"] = None\n",
    "\t\tresults[\"normalized_lcc\"] = None\n",
    "\n",
    "\t\tddc_soup = soup.find(\"ddc\")\n",
    "\t\tif ddc_soup != None:\n",
    "\t\t\tddc_soup = soup.find(\"ddc\").find(\"mostpopular\")\n",
    "\t\t\tif ddc_soup != None:\n",
    "\t\t\t\tif ddc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_ddc\"] = ddc_soup['nsfa']\n",
    "\n",
    "\t\tlcc_soup = soup.find(\"lcc\")\n",
    "\t\tif lcc_soup != None:\n",
    "\t\t\tlcc_soup = soup.find(\"lcc\").find(\"mostpopular\")\n",
    "\t\t\tif lcc_soup != None:\n",
    "\t\t\t\tif lcc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_lcc\"] = lcc_soup['nsfa']\n",
    "\n",
    "\n",
    "\t\tresults[\"headings\"] = []\n",
    "\t\theading_soup = soup.find_all(\"heading\")\n",
    "\t\tfor h in heading_soup:\n",
    "\t\t\tresults[\"headings\"].append({\n",
    "\t\t\t\t\t\"id\" : h['ident'],\n",
    "\t\t\t\t\t\"src\": h['src'],\n",
    "\t\t\t\t\t\"value\" : h.text\n",
    "\t\t\t\t})\n",
    "\t\t\t\n",
    "\n",
    "\t\tedition_soup = soup.find_all(\"edition\")\n",
    "\t\t# print(isbn,len(edition_soup))\n",
    "\t\tresults[\"editions\"] = []\n",
    "\t\tfor e in edition_soup:\n",
    "\t\t\tedition = {}\n",
    "\t\t\tedition['author'] = None if e.has_attr('author') == False else e['author']\n",
    "\t\t\tedition['eholdings'] = None if e.has_attr('eholdings') == False else int(e['eholdings'])\n",
    "\t\t\tedition['format'] = None if e.has_attr('format') == False else e['format']\n",
    "\t\t\tedition['holdings'] = None if e.has_attr('holdings') == False else int(e['holdings'])\n",
    "\t\t\tedition['itemtype'] = None if e.has_attr('itemtype') == False else e['itemtype']\n",
    "\t\t\tedition['language'] = None if e.has_attr('language') == False else e['language']\n",
    "\t\t\tedition['oclc'] = None if e.has_attr('oclc') == False else e['oclc']\n",
    "\t\t\tedition['title'] = None if e.has_attr('title') == False else e['title']\n",
    "\t\t\tresults[\"editions\"].append(edition)\n",
    "\t\t\n",
    "\t\tif len(results[\"editions\"]) > 0:\n",
    "\t\t\tresults[\"largest_holding_oclc\"] = results[\"editions\"][0]['oclc']\n",
    "\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "\n",
    "# run our function over all of them\n",
    "df = df.apply(lambda d: parse_classify_row(d),axis=1 )  \n",
    "# # overwrite back out\n",
    "df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
