{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hathi Download MARC\n",
    "\n",
    "This script adds the MARC xml blob from the Hathi API to a TSV file, it is a preparatory script to run other scripts to extract metadata from the MARC data. \n",
    "\n",
    "It is a requirement that you have either the Hathi Record ID value (numeric id) or the HathiTrust Volume ID (xzy.123456789) in the data \n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It creates a new column in the file `hathi_marc` which holds the MARC XML and `hathi_rights` which also adds the rights code for the title based on all volumes, if there is a mix of rights codes all will be included delimited with a pipe \"|\" character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`id_column_name` - the name of the column header that contains the hathi trust record id or valoume id, you can use either\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/nyt_hardcover_fiction_bestsellers-hathitrust_metadata.tsv\"\n",
    "id_column_name = \"htid\"\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hathi(d):\n",
    "\n",
    "    field = \"htid\"\n",
    "    if type(d[id_column_name]) == int:\n",
    "        field='recordnumber'\n",
    "\n",
    "\n",
    "    # if there is already a value skip it\n",
    "    if 'hathi_marc' in d:\n",
    "        if type(d['hathi_marc']) == str:        \n",
    "            print('Skip',d[id_column_name])\n",
    "            return d\n",
    "        \n",
    "    url = f\"https://catalog.hathitrust.org/api/volumes/full/{field}/{d[id_column_name]}.json\"\n",
    "    r = requests.get(url, headers={'Accept': 'application/json', 'User-Agent': user_agent})\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except:\n",
    "        print(\"JSON decode error with:\",d[id_column_name])\n",
    "        return d\n",
    "\n",
    "    if 'records' not in data:\n",
    "        print(\"No record response found in:\",d[id_column_name])\n",
    "        return d\n",
    "\n",
    "    for recordid in data['records']:        \n",
    "        d['hathi_marc'] = data['records'][recordid]['marc-xml']\n",
    "\n",
    "    if 'items' not in data:\n",
    "        print(\"No items response found in:\",d[id_column_name])\n",
    "        return d\n",
    "\n",
    "    rights_codes = []\n",
    "    for item in data['items']:\n",
    "        rights_codes.append(item['rightsCode'])\n",
    "\n",
    "    rights_codes = list(set(rights_codes))\n",
    "    code = \"|\".join(rights_codes)\n",
    "    \n",
    "    d['hathi_rights'] = code\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_hathi(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This last block just does QA on the data to see what if any rows were not populated\n",
    "\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "print(\"There are \", df['hathi_marc'].isnull().any().sum(), 'rows with no hathi_marc column populated, here are their', id_column_name, 'values:')\n",
    "res = df.loc[df['hathi_marc'].isnull(), id_column_name].tolist()\n",
    "print(print(res))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
