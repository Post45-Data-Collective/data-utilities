{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will be using OCLC Classify service to reconcile bibliographic titles and authors. In this situation we only have the title of the work and the author’s name. Our goal is to enrich the dataset with unique persistent identifiers for the names and titles provided. The approach we are taking in getting these identifiers is to start with the Work, which is the highest level of hierarchy. You can imagine a Work as having many instances, or editions that belong to the same work. So in order to get those instance level information we need to reconcile the Work to the OCLC Classify Work. \n",
    "\n",
    "\n",
    "Getting started checklist:\n",
    "1. A CSV or TSV file with metadata, at minimum it needs to contain the author’s full name and the full title of the work.\n",
    "2. A OCLC WSkey for the Classify service, this can be generated if you have a subscription/membership with OCLC. You need to speak with someone at your institution who has access to your organization’s account at http://platform.worldcat.org/wskey/\n",
    "3. Python, with Pandas, Requests and BeautifulSoup module installed and internet connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first steps will be defining some variables we will be using, set these variables below based on your setup:\n",
    "\n",
    "\n",
    "`path_to_tsv` - the path to the TSV/CSV file you want to run it on\n",
    "\n",
    "`id_author_name` - the name of the column header in the file that contains author's name\n",
    "\n",
    "`id_title_name` - the name of the column header in the file that contains title of the work\n",
    "\n",
    "`id_author_viaf` - the name of the column header that the author's viaf number will be put into\n",
    "\n",
    "`id_author_authorized_heading` - the name of the column header that the  author's authorized heading string\n",
    "\n",
    "`id_author_lccn`- the name of the column header that the author's lccn number\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call, if you want the script to run slower\n",
    "\n",
    "`WSKey` - the OCLC WSkey, will be a 80 character alpha numeric code\n",
    "\n",
    "We also will load the modules we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/path/to/the_file.tsv\"\n",
    "id_author_name = 'author'\n",
    "id_title_name = 'title'\n",
    "id_author_viaf  = 'author_viaf'\n",
    "id_author_authorized_heading  = 'author_authorized_heading'\n",
    "id_author_lccn = 'author_lccn'\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n",
    "WSkey = \"WSKeyXXXXXXXXXXXXXX\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working with the file we need to define two helper funcions. The first will be what each row of the file is run throuh to modify the values. The second is a helper function to parse the results returned from the Classify service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classify(d):\n",
    "\n",
    "    # You can add some logic here to skip rows that already have some data if you previously ran it\n",
    "    # if 'oclc_classify' in d:\n",
    "    #     if type(d['oclc_classify']) == str:        \n",
    "    #         print('Skip',d[id_author_name])\n",
    "    #         return d\n",
    "\n",
    "\n",
    "    # make the call out to the classift service\n",
    "    headers = {'X-OCLC-API-Key': WSkey}\n",
    "    params = {'author': d[id_author_name], 'title': d[id_title_name], 'summary' : 'false', 'maxRecs':100}\n",
    "    r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "    work_parsed = None\n",
    "    work_unparsed = None\n",
    "\n",
    "    # different response codes mean different things:\n",
    "    # 0:\tSuccess. Single-work summary response provided.\n",
    "    # 2:\tSuccess. Single-work detail response provided.\n",
    "    # 4:\tSuccess. Multi-work response provided.\n",
    "    # 100:\tNo input. The method requires an input argument.\n",
    "    # 101:\tInvalid input. The standard number argument is invalid.\n",
    "    # 102:\tNot found. No data found for the input argument.\n",
    "    # 200:\tUnexpected error.\n",
    "\n",
    "    if r.text.find('<response code=\"2\"/>') > -1:\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"4\"/>') > -1:\n",
    "        \n",
    "        # we need to look through this reponse since it returned multiple possiblities, we will just be selecting the one with the most holdings\n",
    "        soup = BeautifulSoup(str(r.text))\n",
    "        work_soup = soup.find(\"works\")\n",
    "        largest_count = 0\n",
    "        largest_work = None\n",
    "        for work in work_soup.find_all(\"work\"):\n",
    "            if int(work['holdings']) > largest_count:\n",
    "                largest_count = int(work['holdings'])\n",
    "                largest_work = work\n",
    "\n",
    "        # once we have that one we want to use, make the request again with its ID now\n",
    "        params = {'owi': largest_work['owi'], 'summary' : 'false', 'maxRecs':100}\n",
    "        r = requests.get('https://metadata.api.oclc.org/classify/', params=params,headers=headers)\n",
    "\n",
    "        work_parsed = extract_classify(r.text)\n",
    "        work_unparsed = r.text\n",
    "\n",
    "    elif r.text.find('<response code=\"100\"/>') > -1 or r.text.find('<response code=\\\\\"100\\\\\"/>') >-1:\n",
    "        print(params,'100: No input. The method requires an input argument.')\n",
    "    elif r.text.find('<response code=\"101\"/>') > -1 or r.text.find('<response code=\\\\\"101\\\\\"/>') >-1:\n",
    "        print(params,'101: Invalid input. The standard number argument is invalid.')\n",
    "    elif r.text.find('<response code=\"102\"/>') > -1 or r.text.find('<response code=\\\\\"102\\\\\"/>') >-1:\n",
    "        print(params,'102: ?.')\n",
    "    elif r.text.find('<response code=\"200\"/>') > -1 or r.text.find('<response code=\\\\\"200\\\\\"/>') >-1:\n",
    "        print(params,'200: Unexpected error.')\n",
    "    else:\n",
    "        print(\"unknown Problem:\",r.text)\n",
    "\n",
    "\n",
    "    if work_parsed != None:\n",
    "        # we are going to store the raw XML from the response in the file as well as the extracted information\n",
    "        d['oclc_classify'] = work_unparsed\n",
    "\n",
    "        # check if the columns we want to add are not yet there in the row, if not add them as null\n",
    "        if id_author_viaf not in d:\n",
    "            d[id_author_viaf] = None\n",
    "\n",
    "        if id_author_authorized_heading not in d:\n",
    "            d[id_author_authorized_heading] = None\n",
    "\n",
    "        if id_author_lccn not in d:\n",
    "            d[id_author_lccn] = None\n",
    "\n",
    "        # add in the author info if is missing\n",
    "        if pd.isnull(d[id_author_viaf]) == True and work_parsed['work_author'] != None:\n",
    "            if work_parsed['work_author']['viaf'] != None:\n",
    "                d[id_author_authorized_heading] = work_parsed['work_author']['name']\n",
    "                d[id_author_viaf] = work_parsed['work_author']['viaf']\n",
    "        \n",
    "        if pd.isnull(d[id_author_lccn]) == True and work_parsed['work_author'] != None:\n",
    "            if work_parsed['work_author']['lccn'] != None:                    \n",
    "                d[id_author_lccn] = work_parsed['work_author']['lccn']\n",
    "                d[id_author_authorized_heading] = work_parsed['work_author']['name']\n",
    "\n",
    "        d['oclc_eholdings'] = data['work_eholdings']\n",
    "        d['oclc_holdings'] = data['work_holdings']\n",
    "        d['oclc_owi'] = data['work_owi']\n",
    "\n",
    "\n",
    "    # if we need to script to run slower we can configure it setting the  pause_between_req variable above \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second helper function, the Classify service returns a XML blob with data that can be parsed. We use the BeautifulSoup module to help read a parse the data.\n",
    "The function returns a dictonary with differnt keys:\n",
    "\n",
    "`work_statement_responsibility` - the string statement of responsibility\n",
    "\n",
    "`work_editions` - the total number of editions this work has\n",
    "\n",
    "`work_eholdings` - the total eletronic holdings this work has\n",
    "\n",
    "`work_format` - what format the work is, likely \"Book\"\n",
    "\n",
    "`work_holdings` - how many instances of this work exists in all the OCLC membership libraries\n",
    "\n",
    "`work_itemtype` - likely \"itemtype-book\"\n",
    "\n",
    "`work_owi` - a Work identifier from OCLC, mostly only used inside this Classify service\n",
    "\n",
    "`work_title` - title\n",
    "\n",
    "`authors` - A list of dictonaries that contain `name` `lccn` `viaf` for each contributor\n",
    "\n",
    "`work_author` - the \"main\" contribtor, like the auhtor opposed to illustrator or other contributor\n",
    "\n",
    "`normalized_ddc` - the most common dewey decimal value for this work\n",
    "\n",
    "`normalized_lcc` - the most common library of congress classifciation number for this work\n",
    "\n",
    "`editions` - a list of dictonaries for all the editions (instances) that represent this work in OCLC insitutions, each dict contains: `author` `eholdings` `format` `holdings` `itemtype` `language` `oclc` `title`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify(xml):\n",
    "\n",
    "\t\tsoup = BeautifulSoup(str(xml))\n",
    "\n",
    "\t\twork_soup = soup.find(\"work\")\n",
    "\n",
    "\t\tif work_soup == None:\n",
    "\t\t\t# print(\"can not parse xml:\")\n",
    "\t\t\t# print(xml)\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tresults = {}\n",
    "\n",
    "\t\tresults['work_statement_responsibility'] = None if work_soup.has_attr('author') == False else work_soup['author']\n",
    "\t\tresults['work_editions'] = None if work_soup.has_attr('editions') == False else int(work_soup['editions'])\n",
    "\t\tresults['work_eholdings'] = None if work_soup.has_attr('eholdings') == False else int(work_soup['eholdings'])\n",
    "\t\tresults['work_format'] = None if work_soup.has_attr('format') == False else work_soup['format']\n",
    "\t\tresults['work_holdings'] = None if work_soup.has_attr('holdings') == False else int(work_soup['holdings'])\n",
    "\t\tresults['work_itemtype'] = None if work_soup.has_attr('itemtype') == False else work_soup['itemtype']\n",
    "\t\tresults['work_owi'] = None if work_soup.has_attr('owi') == False else work_soup['owi']\n",
    "\t\tresults['work_title'] = None if work_soup.has_attr('title') == False else work_soup['title']\n",
    "\t\tresults['main_oclc'] = work_soup.text\n",
    "\n",
    "\t\t\t\t\n",
    "\t\tauthors_soup = soup.find_all(\"author\")\n",
    "\t\tresults['authors'] = []\n",
    "\t\tfor a in authors_soup:\n",
    "\t\t\tresults['authors'].append({\n",
    "\t\t\t\t\t\"name\" : a.text.split('[')[0].strip(),\n",
    "\t\t\t\t\t\"lccn\" : None if a.has_attr('lc') == False else a['lc'],\n",
    "\t\t\t\t\t\"viaf\" : None if a.has_attr('viaf') == False else a['viaf']\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\tfor a in results['authors']:\n",
    "\t\t\tif a['lccn'] == 'null':\n",
    "\t\t\t\ta['lccn'] = None\t\n",
    "\t\t\tif a['viaf'] == 'null':\n",
    "\t\t\t\ta['viaf'] = None\t\n",
    "\n",
    "\t\t# try to find the first main contributor\n",
    "\t\tresults['work_author'] = None\n",
    "\t\tif results['work_statement_responsibility'] != None:\n",
    "\t\t\tif len(results['work_statement_responsibility'].split(\"|\"))>0:\n",
    "\t\t\t\tfirst_author = results['work_statement_responsibility'].split(\"|\")[0].strip()\n",
    "\t\t\t\tfor a in results['authors']:\n",
    "\t\t\t\t\tprint(a['name'].split('[')[0].strip(), first_author )\n",
    "\t\t\t\t\tif a['name'].strip() == first_author:\n",
    "\t\t\t\t\t\tresults['work_author'] = a\n",
    "\n",
    "\n",
    "\n",
    "\t\tresults[\"normalized_ddc\"] = None\n",
    "\t\tresults[\"normalized_lcc\"] = None\n",
    "\n",
    "\t\tddc_soup = soup.find(\"ddc\")\n",
    "\t\tif ddc_soup != None:\n",
    "\t\t\tddc_soup = soup.find(\"ddc\").find(\"mostpopular\")\n",
    "\t\t\tif ddc_soup != None:\n",
    "\t\t\t\tif ddc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_ddc\"] = ddc_soup['nsfa']\n",
    "\n",
    "\t\tlcc_soup = soup.find(\"lcc\")\n",
    "\t\tif lcc_soup != None:\n",
    "\t\t\tlcc_soup = soup.find(\"lcc\").find(\"mostpopular\")\n",
    "\t\t\tif lcc_soup != None:\n",
    "\t\t\t\tif lcc_soup.has_attr('nsfa'):\n",
    "\t\t\t\t\tresults[\"normalized_lcc\"] = lcc_soup['nsfa']\n",
    "\n",
    "\n",
    "\t\tresults[\"headings\"] = []\n",
    "\t\theading_soup = soup.find_all(\"heading\")\n",
    "\t\tfor h in heading_soup:\n",
    "\t\t\tresults[\"headings\"].append({\n",
    "\t\t\t\t\t\"id\" : h['ident'],\n",
    "\t\t\t\t\t\"src\": h['src'],\n",
    "\t\t\t\t\t\"value\" : h.text\n",
    "\t\t\t\t})\n",
    "\t\t\t\n",
    "\n",
    "\t\tedition_soup = soup.find_all(\"edition\")\n",
    "\t\t# print(isbn,len(edition_soup))\n",
    "\t\tresults[\"editions\"] = []\n",
    "\t\tfor e in edition_soup:\n",
    "\t\t\tedition = {}\n",
    "\t\t\tedition['author'] = None if e.has_attr('author') == False else e['author']\n",
    "\t\t\tedition['eholdings'] = None if e.has_attr('eholdings') == False else int(e['eholdings'])\n",
    "\t\t\tedition['format'] = None if e.has_attr('format') == False else e['format']\n",
    "\t\t\tedition['holdings'] = None if e.has_attr('holdings') == False else int(e['holdings'])\n",
    "\t\t\tedition['itemtype'] = None if e.has_attr('itemtype') == False else e['itemtype']\n",
    "\t\t\tedition['language'] = None if e.has_attr('language') == False else e['language']\n",
    "\t\t\tedition['oclc'] = None if e.has_attr('oclc') == False else e['oclc']\n",
    "\t\t\tedition['title'] = None if e.has_attr('title') == False else e['title']\n",
    "\t\t\tresults[\"editions\"].append(edition)\n",
    "\t\t\n",
    "\t\tif len(results[\"editions\"]) > 0:\n",
    "\t\t\tresults[\"largest_holding_oclc\"] = results[\"editions\"][0]['oclc']\n",
    "\n",
    "\t\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to load the Pandas module and load the data we are using, you can adjust the `sep` argument to change what delimiter is being used (for example if you are using a CSV file, change it to \",\"). Once loaded we pass each record to the `add_classify()` function to kick off adding the data to the record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "df = df.apply(lambda d: add_classify(d),axis=1 )  \n",
    "\n",
    "# we are writing out the file to the same location here, you may want to modifythe filename to create a new file, and change the sep argument if using a CSV\n",
    "df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does the same thing as the block above but it breaks the CSV/TSV into multiple chunks and writes it out after each chunk, this allows for recovery from any errors such as as internet timeout or other problems that would cause you to loose all progress unless the script runs flawlessly, you would likely want to use this approch for larger datasets. Also uncomment the starting check in add_classify:\n",
    "```\n",
    "def add_classify(d):\n",
    "\n",
    "    # You can add some logic here to skip rows that already have some data if you previously ran it\n",
    "    # if 'oclc_classify' in d:\n",
    "    #     if type(d['oclc_classify']) == str:        \n",
    "    #         print('Skip',d[id_author_name])\n",
    "    #         return d\n",
    "\n",
    "```\n",
    "\n",
    "To allow it to check if it needs to skip the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 10:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_classify(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
