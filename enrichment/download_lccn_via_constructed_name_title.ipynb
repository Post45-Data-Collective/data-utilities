{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author LCCN Download\n",
    "\n",
    "This script will talk to id.loc.gov to find the LCCN for a name and title \n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It creates a new column in the file `author_lccn` with the LCCN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/major_literary_prizes-winners_judges.tsv\"\n",
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "pause_between_req = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lccn(d):\n",
    "    \n",
    "       \n",
    "    print(d['full_name'])\n",
    "    \n",
    "    if there is already a value skip it\n",
    "    if 'author_lccn' in d:\n",
    "        if type(d['author_lccn']) == str:        \n",
    "            print('Skip already has lccn',d['author_lccn'], d['full_name'])\n",
    "            return d\n",
    "\n",
    "    if type(d['title_of_winning_book']) != str:        \n",
    "        print('Skip - no title',d['full_name'], d['title_of_winning_book'])\n",
    "        return d\n",
    "\n",
    "    if d['role'] != 'winner':        \n",
    "        print('Skip - not winner ',d['full_name'], d['role'])\n",
    "        return d\n",
    "\n",
    "    if d['full_name'] == 'No Winner':        \n",
    "        print('Skip - No Winner',d['full_name'])\n",
    "        return d\n",
    "    \n",
    "  \n",
    "    name = f\"{d['last_name']}, {d['given_name']}\"\n",
    "\n",
    "    # drop any trailing commas or periods\n",
    "    if name[-1] == '.'  or name[-1] == ',':\n",
    "        name = name[:-1]\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'q' : name,\n",
    "        'count': 5\n",
    "    }\n",
    "    headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "    url = f\"https://id.loc.gov/authorities/names/suggest2/\"\n",
    "\n",
    "    r = requests.get(url,params=params,headers=headers)\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except:\n",
    "        print(\"JSON decode error with:\",d['full_name'])\n",
    "        return d            \n",
    "\n",
    "    results = data['hits']\n",
    "\n",
    "    # if we are here then no match, loop again and look at the titles if enabled\n",
    "    for hit in results:\n",
    "        url = 'https://id.loc.gov/resources/works/relationships/contributorto/'\n",
    "        params = {\n",
    "            'page': 0,\n",
    "            'label':hit['aLabel']\n",
    "        }\n",
    "        headers={'Accept': 'application/json', 'User-Agent': user_agent}\n",
    "\n",
    "        r = requests.get(url,params=params,headers=headers)\n",
    "        try:\n",
    "            title_data = r.json()\n",
    "        except:\n",
    "            print(\"JSON decode error with:\",d['full_name'])\n",
    "            return d\n",
    "\n",
    "        if title_data['results'] != None:\n",
    "            # convert it to a list if it a single result dictonary\n",
    "            if type(title_data['results']) != list:\n",
    "                title_data['results'] = [title_data['results']]\n",
    "            for title in title_data['results']:\n",
    "                if normalize_string(d['title_of_winning_book']) in normalize_string(title['label']):\n",
    "                    # we found the title hit, use this one\n",
    "                    d['author_lccn'] = hit['uri'].split('/')[-1]\n",
    "                    d['match_score'] = 'id title match'\n",
    "                    \n",
    "                    print(\"Found!\", d['title_of_winning_book'], 'in', title['label'], ' for ', d['full_name'] )\n",
    "                    return d\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"No results for \",d['full_name'])\n",
    "    \n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "    return d\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.lower()\n",
    "    s = s.casefold()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = s.replace('the','')\n",
    "    return s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 100  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "\n",
    "    # if you want it to skip X number of chunks uncomment this, the number is the row to skip to\n",
    "    # if idx < 88:\n",
    "    #     continue\n",
    "\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_lccn(d),axis=1 )  \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
