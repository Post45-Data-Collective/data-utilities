{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata Download - Reconcile Publisher\n",
    "\n",
    "This script adds the Wikidata Q number from the Wikdiata API to a TSV file for the publisher\n",
    "\n",
    "This script modifies the TSV file itself in batches, should the script timeout or other error you can rerun it and it will pickup where it left off, always run it on a backup of your orginal data files.\n",
    "\n",
    "It expects there to be a publisher name, you can extract that from the MARC with the script`parse_marc_add_publisher.ipynb`\n",
    "\n",
    "It creates a new column in the file `wikidata_publisher_qid` `wikidata_publisher_name` which holds the wikidata QId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id_column_author_qid## Config\n",
    "Set these variables below based on your setup\n",
    "\n",
    "`path_to_tsv` - the path to the TSV file you want to run it on\n",
    "\n",
    "`publisher_name_column` - the name of the column header that contains publisher name\n",
    "\n",
    "`user_agent` - this is the value put into the headers on each request, it is good practice to identifiy your client/project when working with open free APIs\n",
    "\n",
    "`pause_between_req` - number of seconds to wait between each API call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tsv = \"/Users/m/Downloads/data-tmp/hathitrust_post45fiction_metadata.tsv\"\n",
    "\n",
    "publisher_name_column = \"publisher_marc\"\n",
    "user_agent = 'USER YOUR_USER_NAME - Test Script'\n",
    "pause_between_req = 0\n",
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qid(d):\n",
    "\n",
    "\n",
    "\n",
    "    # if there is already a value skip it\n",
    "    if 'wikidata_publisher_qid' in d:\n",
    "        if type(d['wikidata_publisher_qid']) == str:        \n",
    "            # print('Skip',d[id_column_name])\n",
    "            return d\n",
    "\n",
    "    if pd.isnull(d[publisher_name_column]) == True:\n",
    "        return d\n",
    "\n",
    "    if d[publisher_name_column] in cache:\n",
    "        if cache[d[publisher_name_column]] == None:\n",
    "            return d\n",
    "        else:                \n",
    "            d['wikidata_publisher_name'] = cache[d[publisher_name_column]]['name']\n",
    "            d['wikidata_publisher_qid'] = cache[d[publisher_name_column]]['qid']\n",
    "            return d\n",
    "\n",
    "\n",
    "    # use the full search by default\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        'action':'query',\n",
    "        'srsearch':d[publisher_name_column],\n",
    "        'format':'json',\n",
    "        'list':'search',\n",
    "        'srlimit':'10'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    # make a list of the qids to use\n",
    "    qids = []\n",
    "    for s in data['query']['search']:\n",
    "        qids.append(s['title'])\n",
    "\n",
    "    total_hits = data['query']['searchinfo']['totalhits']\n",
    "\n",
    "\n",
    "    if len(qids) == 0:\n",
    "        return d\n",
    "\n",
    "    # build the SPARQL query we are going to use\n",
    "    qids_with_quotes = []\n",
    "    for q in qids:\n",
    "        qids_with_quotes.append(f'wd:{q}')\n",
    "\n",
    "    sparql = f\"\"\"\n",
    "        SELECT ?item ?itemLabel ?instance ?instanceLabel\n",
    "        WHERE \n",
    "        {{\n",
    "\n",
    "            VALUES ?item {{ { \" \".join(qids_with_quotes)  }}}\n",
    "\n",
    "            ?item wdt:P31 ?instance.\n",
    "          \n",
    "            # optional{{\n",
    "            #   ?item wdt:P106 ?occupation.\n",
    "            # }}\n",
    "            # optional{{\n",
    "            #   ?item wdt:P569 ?birth.\n",
    "            # }}\n",
    "            # optional{{\n",
    "            #   ?item wdt:P570 ?death.\n",
    "            # }}          \n",
    "            # optional{{\n",
    "            #   ?item wdt:P166 ?award.\n",
    "            # }}    \n",
    "            # optional{{\n",
    "            #   ?item wdt:P214 ?viaf.\n",
    "            # }}              \n",
    "            # optional{{\n",
    "            #   ?item wdt:P244 ?lccn.\n",
    "            # }}\n",
    "            # optional{{\n",
    "            #   ?item wdt:P69 ?education.\n",
    "            # }}\n",
    "\n",
    "\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'query' : sparql\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Accept' : 'application/json',\n",
    "        'User-Agent': user_agent\n",
    "    }\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    # print(sparql)\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    publishers = {}\n",
    "\n",
    "    # did we get any results\n",
    "    if len(data['results']['bindings']) > 0:\n",
    "      for result in data['results']['bindings']:\n",
    "\n",
    "        qid = result['item']['value'].split('/')[-1]\n",
    "        \n",
    "\n",
    "        if qid not in publishers:\n",
    "\n",
    "          publishers[qid] = {\n",
    "            'qid':qid,\n",
    "            'score':100,\n",
    "            'score_log':[],\n",
    "            'instance':[],\n",
    "            'name': result['itemLabel']['value']\n",
    "\n",
    "            \n",
    "          }\n",
    "        \n",
    "        if 'instanceLabel' in result:\n",
    "          publishers[qid]['instance'].append(result['instanceLabel']['value'])\n",
    "          publishers[qid]['instance'] = list(set(publishers[qid]['instance']))\n",
    "       \n",
    "      \n",
    "\n",
    "    # they must have a writerly occupation to continue\n",
    "    # print(publishers)\n",
    "    for p in publishers:\n",
    "\n",
    "      is_book_publisher = False      \n",
    "      for instance in ['publisher', 'book publisher', 'imprint']:\n",
    "        if instance in publishers[p]['instance']:\n",
    "          is_book_publisher = True\n",
    "\n",
    "    \n",
    "      if is_book_publisher == False:\n",
    "        # print(publishers[p]['name'], '==', d[publisher_name_column] )\n",
    "        publishers[p]['score'] = -1000\n",
    "\n",
    "      publishers[p]['score'] = publishers[p]['score'] - levenshtein(normalize_string(publishers[p]['name']),normalize_string(d[publisher_name_column]))\n",
    "    \n",
    "    best_publisher_score = 0\n",
    "    best_publisher = None\n",
    "    for p in publishers:\n",
    "      if publishers[p]['score'] > best_publisher_score:\n",
    "        best_publisher_score = publishers[p]['score']\n",
    "        best_publisher = publishers[p]\n",
    "    \n",
    "    if best_publisher != None:\n",
    "\n",
    "        d['wikidata_publisher_name'] = best_publisher['name']\n",
    "        d['wikidata_publisher_qid'] = best_publisher['qid']\n",
    "\n",
    "        cache[d[publisher_name_column]] = best_publisher\n",
    "        print(\"For \",d[publisher_name_column], 'Best match is:', best_publisher['name'], \"Cache:\", len(cache))\n",
    "\n",
    "\n",
    "    else:\n",
    "        cache[d[publisher_name_column]] = None\n",
    "\n",
    "    time.sleep(pause_between_req)\n",
    "\n",
    "\n",
    "\n",
    "    return d\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    s = s.lower()\n",
    "    s = s.casefold()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    s = s.replace('the','')\n",
    "    return s.strip()\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsv\n",
    "df = pd.read_csv(path_to_tsv, sep='\\t', header=0, low_memory=False)\n",
    "\n",
    "\n",
    "# we are going to split the dataframe into chunks so we can save our progress as we go but don't want to save the entire file on on every record operation\n",
    "n = 500  #chunk row size\n",
    "list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "# loop through each chunk\n",
    "for idx, df_chunk in enumerate(list_df):\n",
    "\n",
    "    # if idx < 25:\n",
    "    #     continue\n",
    "\n",
    "    print(\"Working on chunk \", idx, 'of', len(list_df))\n",
    "    list_df[idx] = list_df[idx].apply(lambda d: add_qid(d),axis=1 )  \n",
    "    \n",
    "\n",
    "    reformed_df = pd.concat(list_df)\n",
    "    reformed_df.to_csv(path_to_tsv, sep='\\t')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
